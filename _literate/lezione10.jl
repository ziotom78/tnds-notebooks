# # Lezione 10
#
# Iniziamo importando i pacchetti che ci serviranno.

using Printf
using Plots
using Statistics

# ## Esercizio 10.0
#
# Definiamo una classe `GLC` che sia equivalente alla classe `Random`
# che vi viene richiesto di implementare in C++.
#
# In Julia non esiste il concetto di ¬´classe¬ª, ma esistono le `struct`
# che funzionano in modo concettualmente simile. Non permettono di
# associare metodi, tranne eventualmente un semplice costruttore, e
# tutti i campi sono pubblici di default.

# ### Generatore Lineare Congruenziale

mutable struct GLC
    a::UInt32
    c::UInt32
    m::UInt32
    seed::UInt32

    GLC(myseed) = new(1664525, 1013904223, 1 << 31, myseed)
end

# Il tipo `UInt32` corrisponde a `unsigned int` in C++.
#
# La strana scrittura `1 << 31` √® un'operazione di [bit
# shift](https://en.wikipedia.org/wiki/Bitwise_operation#Bit_shifts):
# dice di considerare il numero `1` in binario, e di spostarlo a
# sinistra, aggiungendo quindi alla sua destra tanti zeri quanti il
# secondo operando (31). Ecco alcuni esempi, dove i numeri che
# iniziano con `0b` sono scritti in binario (√® una convenzione del C++
# e di Julia):
#
# ```text
# 0b10010 << 1 == 0b100100    (uno zero aggiunto alla fine)
# 0b10010 << 3 == 0b10010000  (tre zeri aggiunti alla fine)
# 0b10010 >> 2 == 0b100       (due cifre tolte alla fine)
# ```
#
# Potete comprendere il significato dell'operazione se pensate al caso
# decimale: se sposto un numero come `1` a sinistra, aggiungendo 31
# zeri, lo sto moltiplicando per $10^{31}$, ottenendo quindi il numero
# `1e+31`. Analogamente, se tolgo $N$ cifre a destra di un numero, lo
# sto *dividendo* per $10^N$.
#
# Nel caso binario, `1 << 31` vuol dire moltiplicare `1` per
# $2^{31}$, ma quest'operazione √® molto pi√π rapida che usando `pow()`
# in C++ o l'operatore `^` in Julia, perch√© il bit-shift viene fatto a
# livello di singoli capacitori e induttanze nella CPU, che
# ‚Äútravasano‚Äù la carica di un bit nel bit accanto, ed √® un'operazione
# velocissima.

#md # !!! note "Piccola nota storica"
#md #     Negli anni '90 il compilatore [Borland C++](https://en.wikipedia.org/wiki/Borland_C%2B%2B) aveva introdotto l'ottimizzazione di tradurre istruzioni come `x *= 2` in `x <<= 1`, e analogamente per la divisione intera per 2 o sue potenze. Questo aveva causato un sensibile aumento di velocit√† di certi codici, che la Borland aveva pubblicizzato nelle sue brochures! Oggi quest'ottimizzazione √® diventata standard su tutti i compilatori, non solo C++, ma all'epoca era un trucco da ‚Äúaddetti ai lavori‚Äù, ed aveva suscitato molto interesse il fatto che un compilatore fosse diventato cos√¨ furbo da saperla applicare in certi casi.

# Definiamo ora una funzione `rand` che restituisca un numero casuale
# floating-point compreso in un intervallo:

@doc """
    rand(glc::GLC, xmin, xmax)

Return a pseudo-random number uniformly distributed in the
interval [xmin, xmax).
"""
function rand(glc::GLC, xmin, xmax)
    glc.seed = (glc.a * glc.seed + glc.c) % glc.m
    xmin + (xmax - xmin) * glc.seed / glc.m
end

# √à molto comodo avere anche una funzione `rand` che usi l'intervallo
# $[0, 1]$.

@doc """
    rand(glc::GLC)

Return a pseudo-random number uniformly distributed in the
interval [0, 1).
"""
rand(glc::GLC) = rand(glc, 0.0, 1.0)

# Questi sono i numeri che dovreste aspettarvi se avete implementato bene il
# vostro codice (notate che i numeri cambiano se usate un seed diverso!).

glc = GLC(1)
for i in 1:5
    println(i, ": ", rand(glc))
end

# Preoccupatevi quindi di creare una serie di `assert` nel vostro
# codice C++ che verifichino che ottenete gli stessi valori se partite
# dallo stesso seme (`1`), possibilmente in una funzione
# `test_random_numbers()` invocata all'inizio del vostro `main`.
#
# Quando si implementano numeri pseudo-casuali, √® sempre bene farsi
# un'idea della distribuzione dei valori. Disegnamo quindi
# l'istogramma della distribuzione di un gran numero di campioni, e
# verifichiamo che siano uniformemente distribuiti nell'intervallo [0,
# 1).

histogram([rand(glc) for i in 1:10000], label="");
savefig(joinpath(@OUTPUT, "rand_hist.svg")); # hide

# \fig{rand_hist.svg}

# ### Distribuzione esponenziale
#
# Trattandosi di una formula semplice, in Julia si pu√≤ definire
# `randexp` con una sola riga di codice:

"""
    randexp(glc::GLC)

Return a positive pseudo-random number distributed with a
probability density ``p(x) = Œª e^{-Œª x}``.
"""
randexp(glc::GLC, Œª) = -log(1 - rand(glc)) / Œª

# Questi sono i numeri per i vostri `assert`:

glc = GLC(1)
for i in 1:5
    println(i, ": ", randexp(glc, 1))
end

# Questo √® l'istogramma

histogram([randexp(glc, 1) for i in 1:10000], label="");
savefig(joinpath(@OUTPUT, "randexp_hist.svg")); # hide

# \fig{randexp_hist.svg}

# ### Distribuzione Gaussiana

@doc raw"""
    randgauss(glc::GLC, Œº, œÉ)

Return a pseudo-random number distributed with a probability
density ``p(x) = \frac{1}{\sqrt{2œÄœÉ^2}}
\exp\left(-\frac{(x - Œº)^2}{2œÉ^2}\right)``, using the
Box-M√ºller algorithm.
"""
function randgauss(glc::GLC, Œº, œÉ)
    s = rand(glc)
    t = rand(glc)
    x = sqrt(-2log(s)) * cos(2œÄ * t)
    Œº + œÉ * x
end

# All'interno della funzione, nella riga in cui si assegna il valore a
# `x`, vi sareste potuti aspettare la riga
#
# ```julia
# x = sqrt(-2log(1 - s)) * cos(2œÄ * t)
# ```
#
# con il termine `2log(1 - s)` anzich√© `2log(s)`. I due termini *non* sono uguali, ovviamente, ma la loro distribuzione statistica invece s√¨: in entrambi i casi infatti l'argomento del logaritmo √® distribuito uniformemente tra 0 ed 1. Per√≤ la scrittura `2log(s)` risparmia una sottrazione ed √® quindi lievemente pi√π veloce.

# Questi sono i numeri per i vostri `assert`, assumendo ovviamente che anche voi usiate `log(s)` anzich√© `log(1 - s)`:

glc = GLC(1)
for i in 1:5
    println(i, ": ", randgauss(glc, 2, 1))
end

# Questo √® l'istogramma:

histogram([randgauss(glc, 2, 1) for i in 1:10000], label="");
savefig(joinpath(@OUTPUT, "randgauss_hist.svg")); # hide

# \fig{randgauss_hist.svg}

# ### Distribuzione Gaussiana con metodo Accept-Reject

@doc raw"""
    randgauss_ar(glc::GLC, Œº, œÉ)

Return a pseudo-random number distributed with a probability
density ``p(x) = \frac1{\sqrt{2œÄœÉ^2}}
\exp\left(-\frac{(x - Œº)^2}{2œÉ^2}\right)``, using the
accept-reject algorithm.
"""
function randgauss_ar(glc::GLC, Œº, œÉ)
    while true  # Loop forever
        x = rand(glc, -5., 5.)
        y = rand(glc)
        g = exp(-x^2 / 2)
        y ‚â§ g && return Œº + x * œÉ
    end
end

# Questi sono i numeri per gli `assert`:

glc = GLC(1)
for i in 1:5
    println(i, ": ", randgauss_ar(glc, 2, 1))
end

# Questo √® l'istogramma:

histogram([randgauss_ar(glc, 2, 1) for i in 1:10000], label="");
savefig(joinpath(@OUTPUT, "randgauss_ar_hist.svg")); # hide

# \fig{randgauss_ar_hist.svg}


# ## Esercizio 10.1
#
# L'esercizio √® molto semplice da implementare, ma richiede comunque una
# certa attenzione: bisogna studiare infatti molti casi (ben 12 istogrammi),
# e questo richiede molto ordine e pulizia! Imparare a scrivere codice
# ordinato √® importante soprattutto per il giorno dell'esame: capita spesso
# che nei temi d'esame si chieda di ripetere pi√π volte un calcolo o una
# simulazione, ed √® bene non usare copia-e-incolla ma strutturare il
# codice usando dei cicli `for` e implementando funzioni di supporto
# anzich√© rendere il `main` lungo centinaia di righe.
#
# Iniziamo con l'implementazione di un codice che riempia un vettore
# con i campioni casuali sommati $N$ alla volta:

function computesums!(glc::GLC, n, vec)
    for i in eachindex(vec)
        accum = 0.0
        for k in 1:n
            accum += rand(glc)
        end
        vec[i] = accum
    end
end

# (in Julia c'√® la convenzione di mettere il carattere `!` alla fine delle
# funzioni che modificano uno dei loro argomenti: questo √® proprio il nostro
# caso, perch√© `vec` viene modificato da `computesums!`)
#
# Facciamo una prova semplice:

glc = GLC(1)
## Array di *due* elementi
vec = Array{Float64}(undef, 2)
## Chiediamo che in ogni elemento vengano sommati *cinque*
## numeri. Quindi ogni elemento di `vec` sar√† un numero
## casuale nell'intervallo 0‚Ä¶5.
computesums!(glc, 5, vec)
println("vec[1] = ", vec[1])
println("vec[2] = ", vec[2])

# Potete usare questi numeri in un `assert` per verificare la
# vostra implementazione di `compute_sums` (mettete pure tutto
# nello stesso file del `main`):
#
# ```cpp
# void test_compute_sums() {
#   std::vector<double> vec(2);  // Attenzione, parentesi *tonde* qui!
#
#   RandomGen rng{1};
#   compute_sums(rng, 5, vec);
#   assert(are_close(vec[0], 1.7307902472093701));
#   assert(are_close(vec[1], 1.7124183257110417));
#   cerr << "compute_sums() is correct, hurrah! ü•≥\n";
# }
# ```
#
# Ora ci occorre invocare questa funzione pi√π volte facendo variare $N$
# da 1 a 12, e producendo un istogramma ogni volta.

glc = GLC(1)
vec = Array{Float64}(undef, 100_000)

list_of_N = 1:12
list_of_histograms = []
list_of_sigmas = Float64[]
for n in list_of_N
    computesums!(glc, n, vec)
    push!(list_of_histograms, histogram(vec, bins = 20, title = "N = $n"))
    push!(list_of_sigmas, std(vec))
end
plot(
    list_of_histograms...,
    layout = (3, 4),
    size = (900, 600),
    legend = false,
)
savefig(joinpath(@OUTPUT, "es10_1_histogram.svg")); # hide

# \fig{es10_1_histogram.svg}

# Notate che, grazie alla definizione della funzione `computesums!`,
# il ciclo `for` √® stato ridotto ad appena tre righe. Inoltre proprio
# l'uso del `for` ha evitato quegli orribili copia-e-incolla che
# spesso i docenti trovano nelle correzioni degli esami scritti.
#
# Il seguente √® un esempio di come **non** implementare questo
# esercizio; √® un vero esercizio, consegnato da uno studente pochi
# anni fa. √à una vera e propria ‚Äúgalleria degli orrori‚Äù!
#
# ```cpp
# // üëø NON BASATEVI SU QUESTO CODICE! üëø
#
# std::vector<double> vec(100'000);
#
# // Aargh! Qui scrive di nuovo il numero 100'000 anzich√© usare `ssize(vec)`:
# // cosa succede se poi durante l'esame voleva usare un numero minore
# // per risparmiare tempo? Deve cambiare tutte le occorrenze!
# for(int k{}; k < 100'000; ++k) {
#   vec[k] = 0.0;   // Per giunta qui non usa neppure vec.at(k),
#                   // quindi se riduce il numero 100'000 nella
#                   // definizione di `vec` ma non nel ciclo `for`,
#                   // qui poteva avere un ‚Äúsegmentation fault‚Äù!
#   for(int i{}; i < 1; ++i)
#     vec[k] += rand.Unif(0.0, 1.0);
# }
#
# // Ok, invece che fare una sola figura con 12 grafici sceglie di
# // creare 12 file PNG distinti‚Ä¶ √® pi√π faticoso per√≤ poi
# // controllare i risultati e confrontare gli istogrammi!
# Gnuplot plt1{};
# plt1.redirect_to_svg("n1.png");
# plt1.histogram(vec, 20, "N = 1");
# plt1.show();
#
# // Caso con n = 2
#
# // NOOOO! Tutto quanto segue √® un copia-e-incolla del codice sopra!
# // Terribile!
# for(int k{}; k < 100'000; ++k) {
#   vec[k] = 0.0;
#   for(int i{}; i < 2; ++i)
#     vec[k] += rand.Unif(0.0, 1.0);
# }
#
# Gnuplot plt2{};
# plt3.redirect_to_svg("n3.png");
# plt3.histogram(vec, 20, "N = 3");
# plt3.show();
#
# // Caso con n = 3
# for(int k{}; k < 100'000; ++k) {
#   vec[k] = 0.0;
#   for(int i{}; i < 3; ++i)
#     vec[k] += rand.Unif(0.0, 1.0);
# }
#
# Gnuplot plt3{};
# plt3.redirect_to_svg("n3.png");
# plt3.histogram(vec, 20, "N = 3");
# plt3.show();
#
# // Il codice continua tutto cos√¨‚Ä¶ ci siamo capiti!
# // ‚Ä¶
# ```
#
# Il codice Julia evita di ricorrere ai copia-e-incolla implementando
# una funzione `computesums!` e chiamandola pi√π volte all'interno di
# un ciclo `for`. Questo approccio √® estremamente elegante üòá e ha molti
# vantaggi rispetto al disperato copia-e-incolla del malefico esempio üëø:
#
# -   Ci mettete meno tempo a scriverlo, e in un esame il tempo √® sempre prezioso;
# -   Se scegliete l'approccio ‚Äúcopia-e-incolla‚Äù üëø e vi rendete conto di un
#     errore nel codice che avete appena copiato (ad esempio, una parentesi non chiusa),
#     dovete correggerlo dodici volte‚Ä¶ ma nel caso üòá l'errore va corretto una
#     volta sola! E anche questo √® un bel risparmio di tempo.
# -   Il codice üòá √® pi√π semplice da leggere, e quindi √® pi√π facile individuare
#     errori (ci sono meno posti in cui il problema potrebbe nascondersi)
# -   Se vi rendete conto che il programma ci mette troppo per essere
#     eseguito, e questo vi √® di impiccio perch√© i risultati non vi
#     convincono e prevedete di doverlo eseguire molte volte, √®
#     semplice limitare ad esempio i valori di `N` da esplorare nel
#     codice üòá, limitandovi ad esempio ai primi 5 casi anzich√© a
#     tutti e 12. Nel codice üëø invece, dovete commentare decine di
#     righe di codice, col rischio di commentare qualche variabile
#     importante che vi serve alla fine del programma e che quindi
#     causa errori di compilazione‚Ä¶
#
# Ora creiamo il grafico con l'andamento della deviazione standard (calcolata
# nell'esempio sopra con la funzione `Statistics.std`), memorizzata in
# `list_of_sigmas`:

plot(list_of_N, list_of_sigmas,
     xaxis = :log10, yaxis = :log10, label = "",
     xlabel = "N", ylabel = "Standard deviation œÉ")
savefig(joinpath(@OUTPUT, "es10_1_std.svg")); # hide

# \fig{es10_1_std.svg}

# ## Esercizio 10.2
#
# Questa √® una semplice implementazione dell'integrale della media:

"""
    intmean(glc::GLC, fn, a, b, N)

Evaluate the integral of `fn(x)` in the interval ``[a, b]``
using the mean method with ``N`` points.
"""
function intmean(glc::GLC, fn, a, b, N)
    (b - a) * sum([fn(rand(glc, a, b)) for i in 1:N]) / N
end

# L'integrale *hit-or-miss* √® solo lievemente pi√π complicato:

"""
    inthm(glc::GLC, fn, a, b, fmax, N)

Evaluate the integral of `fn(x)` in the interval ``[a, b]``
using the hit-or-miss method with ``N`` points, assuming that
`fn(x)` assumes values in the range `[0, fmax]`.
"""
function inthm(glc::GLC, fn, a, b, fmax, N)
    hits = 0
    for i in 1:N
        x = rand(glc, a, b)
        y = rand(glc, 0, fmax)
        y ‚â§ fn(x) && (hits += 1)
    end

    hits / N * (b - a) * fmax
end

# Verifichiamo che il codice compili, e che produca un risultato sensato.
# Teniamo presente che $\int_0^{\pi/2} x \sin x\,\mathrm{d}x = 1$; inoltre, siccome
# $x \sin(x)$ √® una funzione limitata in $[0, 1]$, possiamo porre `fmax = œÄ/2` nella
# chiamata a `inthm`:

xsinx(x) = x * sin(x)
println("Integrale (metodo media):", intmean(GLC(1), xsinx, 0, œÄ/2, 100))
println("Integrale (metodo hit-or-miss):", inthm(GLC(1), xsinx, 0, œÄ/2, œÄ/2, 100))

# Implementate degli `assert` che verifichino che ottenete gli stessi
# risultati nella vostra implementazione C++. Come gi√† ricordato
# sopra, fate molta attenzione ad inizializzare il generatore di
# numeri pseudo-casuali con lo stesso seme (`1` in questo caso).
# Notate anche che il codice sopra usa **due** generatori di numeri
# casuali: uno per `intmean` e l'altro per `inthm`. Se voi invece
# ne usate uno solo e chiamate `intmean` e poi `inthm` passando sempre
# quello, anche se avete implementato correttamente entrambi i metodi,
# per `intmean` lo stesso numero ma per `inthm` un numero diverso!
#
# Eseguiamo ora il calcolo per 10.000 volte e facciamone l'istogramma:
# osserviamo che la distribuzione √® approssimativamente una Gaussiana,
# come previsto.

glc = GLC(1)
mean_samples = [intmean(glc, xsinx, 0, œÄ / 2, 100) for i in 1:10_000]
histogram(mean_samples, label="Media")

glc = GLC(1)  # Reset the random generator
mean_hm = [inthm(glc, xsinx, 0, œÄ / 2, œÄ / 2, 100) for i in 1:10_000]
histogram!(mean_hm, label="Hit-or-miss");
savefig(joinpath(@OUTPUT, "mc_integrals.svg")); # hide

# \fig{mc_integrals.svg}

# Passiamo ora al punto 2: calcoliamo 10.000 volte il valore dell‚Äôintegrale
# variando il valore di $N$ e disegnamo gli istogrammi:

list_of_N = [500, 1_000, 5_000, 10_000, 50_000, 100_000]
list_of_plots = []
list_of_errors = []
for N in list_of_N
    let samples = [intmean(glc, xsinx, 0, œÄ / 2, N) for i in 1:10_000]
        push!(list_of_plots, histogram(samples, label="N = $N"))
        push!(list_of_errors, std(samples))
    end
end
plot(list_of_plots..., layout=(3, 2), legend=false);
savefig(joinpath(@OUTPUT, "mc_integrals_varying_N.svg")); # hide

# \fig{mc_integrals_varying_N.svg}

# Questo √® il grafico con l‚Äôandamento dell‚Äôerrore:

scatter(
    list_of_N,
    list_of_errors,
    xlabel = "N",
    ylabel = "Errore",
    xaxis = :log10,
    yaxis = :log10,
)
savefig(joinpath(@OUTPUT, "mc_integrals_err_plot.svg")); # hide

# \fig{mc_integrals_err_plot.svg}

# E questi sono i punti calcolati:

println("N       Errore")
for (cur_n, cur_err) in zip(list_of_N, list_of_errors)
    @printf("%d\t%.5f\n", cur_n, cur_err)
end

# Se l'andamento dell'errore √® della forma $\epsilon(N) = k/\sqrt{N}$, con $N$
# numero di punti, allora nel nostro caso possiamo stimare $k$ immediatamente
# dalla deviazione standard dei valori in `values` mediante la formula $k =
# \sqrt{N} \times \epsilon(N)$:

k_mean = ‚àö100 * std(mean_samples)
k_hm = ‚àö100 * std(mean_hm)

println("K (media) = ", k_mean)
println("K (hit-or-miss) = ", k_hm)

# A questo punto, per rispondere alla domanda del problema, √® sufficiente
# risolvere l'equazione $0.001 = k/\sqrt{N}$ per $N$, ossia $$N =
# \left(\frac{k}{0.001}\right)^2$$.

target_Œµ = 0.001
noptim_mean = round(Int, (k_mean / target_Œµ)^2)
noptim_hm = round(Int, (k_hm / target_Œµ)^2)

println("N (media) = ", noptim_mean)
println("N (hit-or-miss) = ", noptim_hm)

# Per verificare la correttezza del risultato, rifacciamo l'istogramma. Siccome
# ci vuole molto tempo per ottenere il risultato, verifichiamo il risultato solo
# nel caso del metodo della media, e per un numero ridotto di realizzazioni
# (1000 anzich√© 10.000):

glc = GLC(1)
values = [intmean(glc, xsinx, 0, œÄ / 2, noptim_mean) for i in 1:1000]
histogram(values, label="");
savefig(joinpath(@OUTPUT, "mc_intmean.svg")); # hide

# \fig{mc_intmean.svg}

# Il risultato √® effettivamente corretto:

std(values)

# # Lezione 12: Simulazione di un esperimento
#
# L'esercizio di questa lezione √® **estremamente** importante, perch√©
# le tecniche Monte Carlo sono molto diffuse in fisica. (E inoltre
# questo √® un tipo di tema d'esame che ricorre spesso!)
#
# Ne approfitto anche per mostrarvi un modo di scrivere codice che
# espliciti le unit√† di misura e faccia automaticamente un controllo
# dimensionale. In C++ questo sarebbe fattibile usando la
# programmazione template, che non √® stata per√≤ quasi mai usata per lo
# svolgimento degli esercizi; tenete presente nei vostri futuri
# progetti che librerie come
# [mp-units](https://mpusz.github.io/mp-units/latest/),
# [Boost.units](https://www.boost.org/doc/libs/1_65_0/doc/html/boost_units.html),
# [SI](https://github.com/bernedom/SI),
# [units](https://github.com/nholthaus/units), etc. possono essere
# usate per specificare le unit√† di misura di variabili e costanti, e
# per verificarne la consistenza nel proprio codice.
#
# Sfortunatamente, il modo in cui avete scritto programmi in questo
# semestre fa uso della programmazione *object-oriented*, che non √®
# adatta per usare questo genere di librerie (e pi√π in generale per il
# calcolo numerico), perch√© avete dichiarato come `double` tutti i
# parametri di metodi come `Solutore::CercaZeri` o
# `Integral::integrate`, mentre per usare queste librerie C++ avreste
# dovuto definire sia `Solutore` che `integral` come classi template.
# Ad esempio:
#
# ```cpp
# template <typename T, typename Fn>
# class Solutore {
# public:
#   Solutore();
#
#   virtual T CercaZeri(T xmin, T xmax, Fn f,
#                       T prec = 1e-3, int nmax = 100) = 0;
# };
# ```
#
# In questo modo, supponendo di usare la libreria
# [units](https://github.com/nholthaus/units), avreste potuto poi
# passare a `Solutore::CercaZeri` variabili dimensionali, perch√© il
# compilatore avrebbe selezionato il tipo `T` giusto (lunghezza,
# tempo, etc.):
#
# ```cpp
# using namespace units::length;
# using namespace units::time;
#
# Bisezione sol{};
#
# // We find the zero of a function f(x), where x is a length
# auto result1 = sol.CercaZeri(0.5_m, 1.5_m, my_function, 1e-4_m);
#
# // We find the zero of a function g(t), where t is a time
# auto result2 = sol.CercaZeri(10.0_s, 15.0_s, another_function, 1e-2_s);
# ```
#
# Ovviamente, n√© `my_function` n√© `another_function` sarebbero pi√π
# state derivate da `FunzioneBase`, dovendo invece essere funzioni che
# accettano quantit√† delle dimensioni giuste.
#
# (In Julia questo tipo di programmazione √® naturale perch√© in un
# certo senso *tutto* √® un template, e ci√≤ lo rende ideale per il
# calcolo scientifico. Vedremo meglio questo aspetto nel seminario
# ‚Äújolly‚Äù che terr√≤ dopo la sessione di esami per chi √® interessato).
#
# ## Esercizio 12.0
#
# Iniziamo con l'importare la libreria
# [Unitful.jl](https://github.com/PainterQubits/Unitful.jl), che
# implementa le unit√† di misura che ci servono. Importeremo
# esplicitamente quelle unit√† di misura che ci serviranno, perch√© la
# libreria di default non ne importa nessuno (simboli come `m`, `s`,
# `mm`, etc., sono molto usati come nomi di variabili, e sarebbe un
# disastro se venissero tutti importati senza criterio!).

using Unitful
import Unitful: m, cm, mm, nm, s, ¬∞, mrad, @u_str

# I simboli `nm`, `¬∞` e `mrad` sono unit√† di misura che si possono
# usare direttamente nelle definizioni, come `x = 10nm`. La macro
# `@u_str`, terminando con `_str`, indica che pu√≤ essere usata
# aggiungendo `u` dopo le stringhe per specificare le unit√† di misura.
# Questo √® indispensabile per tipi pi√π complessi dei semplici `m`,
# `cm`, `mm`, etc., che richiedano espressioni matematiche, come ad
# esempio i campi elettrici: `E = 10u"N/C"`.

# Definiamo una serie di variabili per le costanti fisiche del problema:

œÉ_Œ∏ = 0.3mrad;       # Avrei potuto scrivere œÉ_Œ∏ = 0.3u"mrad"
Œ∏0_ref = 90¬∞;        # Ugualmente,           Œ∏0_ref = 90u"¬∞"
Aref = 2.7;
Bref = 6e4u"nm^2";   # Qui devo usare `u` perch√© nm¬≤ √® troppo complicato
Œ± = 60.0¬∞;
Œª1 = 579.1nm;
Œª2 = 404.7nm;

# La funzione `n_cauchy` restituisce $n$ supponendo vera la formula di Cauchy.
# La sintassi con un parametro usa i valori di riferimento di $A$ e $B$ scritti
# sopra.

n_cauchy(Œª, A, B) = sqrt(A + B / Œª^2)
n_cauchy(Œª) = n_cauchy(Œª, Aref, Bref)

# La funzione `n` invece restituisce $n$ in funzione della deviazione
# misurata `Œ¥` dal prisma, dove $\alpha$ √® il suo angolo di apertura
# (definito sopra). Siccome la funzione `asin` (arcoseno) restituisce
# il valore in radianti, che √® scomodo da leggere, definiamo `Œ¥` in modo
# che esprima sempre il risultato in gradi: per questo scopo c'√® la
# funzione `uconvert`, che richiede come primo parametro l'unit√† di
# misura di *destinazione* (nel nostro caso gradi, quindi `u"¬∞"`).

n(Œ¥) = sin((Œ¥ + Œ±) / 2) / sin(Œ± / 2)
Œ¥(n) = uconvert(u"¬∞", 2asin(n * sin(Œ± / 2)) - Œ±)

# Queste formule si ricavano banalmente dall'inversione della formula di Cauchy;
# la funzione `A_and_B` calcola contemporaneamente $A$ e $B$, ed √® stata
# definita per comodit√†:

A(Œª1, Œ¥1, Œª2, Œ¥2) = (Œª2^2 * n(Œ¥2)^2 - Œª1^2 * n(Œ¥1)^2) / (Œª2^2 - Œª1^2)
B(Œª1, Œ¥1, Œª2, Œ¥2) = (n(Œ¥2)^2 - n(Œ¥1)^2) / (1/Œª2^2 - 1/Œª1^2)
A_and_B(Œª1, Œ¥1, Œª2, Œ¥2) = (A(Œª1, Œ¥1, Œª2, Œ¥2), B(Œª1, Œ¥1, Œª2, Œ¥2))

# Calcoliamo allora i valori di riferimento di $n(\lambda_1) = n_1$ e
# $n(\lambda_2) = n_2$, supponendo veri i valori di $A$ e $B$ scritti sopra
# r(`A_ref` e `B_ref`):

n1_ref, n2_ref = n_cauchy(Œª1), n_cauchy(Œª2)

# Da $n_1$ e $n_2$ calcoliamo quanto aspettarci per $\delta_1$ e
# $\delta_2$:

Œ¥1_ref, Œ¥2_ref = Œ¥(n1_ref), Œ¥(n2_ref)

# Il vostro codice probabilmente stamper√† angoli in radianti (√® la
# convenzione di `asin` in C++), quindi convertiamo i valori sopra in
# modo che possiate confrontarli col risultato del vostro programma:

println("Œ¥1_ref = ", uconvert(u"rad", Œ¥1_ref))
println("Œ¥2_ref = ", uconvert(u"rad", Œ¥2_ref))

# A questo punto possiamo simulare l'esperimento. La simulazione della
# misura di $\delta_1$ e $\delta_2$ va fatta usando l'approssimazione
# Gaussiana con i valori medi `Œ¥1_ref` e `Œ¥2_ref`, e la deviazione
# standard `œÉ_Œ∏` data dal testo dell'esercizio:

function simulate_experiment(glc, nsim)
    n1_simul = Array{Float64}(undef, nsim)
    n2_simul = Array{Float64}(undef, nsim)

    A_simul = Array{Float64}(undef, nsim)
    ## Here I create an array of values whose measurement unit
    ## must be the same as `Bref`
    B_simul = Array{typeof(Bref)}(undef, nsim)

    for i in 1:nsim
        Œ∏0 = randgauss(glc, Œ∏0_ref, œÉ_Œ∏)
        Œ∏1 = randgauss(glc, Œ∏0_ref + Œ¥1_ref, œÉ_Œ∏)
        Œ∏2 = randgauss(glc, Œ∏0_ref + Œ¥2_ref, œÉ_Œ∏)
        Œ¥1, Œ¥2 = Œ∏1 - Œ∏0, Œ∏2 - Œ∏0
        n1, n2 = n(Œ¥1), n(Œ¥2)
        a, b = A_and_B(Œª1, Œ¥1, Œª2, Œ¥2)

        n1_simul[i] = n1
        n2_simul[i] = n2

        A_simul[i] = a
        B_simul[i] = b
    end

    (n1_simul, n2_simul, A_simul, B_simul)
end

# Ecco i primi 5 valori della simulazione; controllate che siano gli
# stessi che ottenete voi, facendo attenzione di usare come seme `1` e
# che l'ordine in cui chiamate la funzione per generare i numeri
# casuali sia la stessa del codice sopra:
#
#   1. _Prima_ si genera $\theta_0$;
#   2. _Poi_ si generano $\theta_1$ e $\theta_2$.
#
# Nel fare i plot qui sotto mi limito a ripetere l'esperimento 1000
# volte (il testo richiede 10.000 volte). I risultati non cambiano
# molto.

glc = GLC(1)
n1_simul, n2_simul, A_simul, B_simul = simulate_experiment(glc, 1000)

@printf("%14s %14s %14s %14s\n", "n‚ÇÅ", "n‚ÇÇ", "A", "B [nm¬≤]")
println(repeat('-', 62))
for i = 1:5
    ## We use scientific notation for B, as it is ‚â™1. As we want to
    ## avoid printing units for B (they are already in the table header),
    ## we just ¬´strip¬ª nm¬≤ from it.
    @printf("%14.6f %14.6f %14.6f %14.6e\n",
            n1_simul[i], n2_simul[i], A_simul[i], ustrip(u"nm^2", B_simul[i]))
end

#

histogram([n1_simul, n2_simul],
          label = ["n‚ÇÅ" "n‚ÇÇ"],
          layout = (2, 1));
savefig(joinpath(@OUTPUT, "hist_n1_n2.svg")); # hide

# \fig{hist_n1_n2.svg}

scatter(n1_simul, n2_simul, label="");
savefig(joinpath(@OUTPUT, "scatter_n1_n2.svg")); # hide

# \fig{scatter_n1_n2.svg}

# Il package `Statistics` di Julia implementa il calcolo della
# covarianza tra due serie, che √® uguale alla correlazione a meno di
# una normalizzazione. Definiamo quindi la funzione `corr`, che
# calcola il coefficiente di correlazione, analogamente a questa; nel
# vostro codice C++ dovrete invece implementarla usando la formula.

corr(x, y) = cov(x, y) / (std(x) * std(y))

# I valori di $n_1$ ed $n_2$ sono correlati, perch√© sono entrambi
# stati ricavati dalla medesima stima di $\theta_0$.

corr(n1_simul, n2_simul)

# Nel fare l'istogramma di $A$ e $B$, rimuoviamo le unit√† di misura da
# quest'ultimo, perch√© altrimenti Julia segnalerebbe che `A_simul` e
# `B_simul` sono incompatibili (essendo combinati nella stessa
# chiamata ad `histogram`):

histogram([A_simul, ustrip.(u"nm^2", B_simul)],
          label = ["A" "B"],
          layout = (2, 1))
savefig(joinpath(@OUTPUT, "hist_A_B.svg")); # hide

# \fig{hist_A_B.svg}

# Facciamo anche un grafico X-Y

scatter(A_simul, B_simul, label="");
savefig(joinpath(@OUTPUT, "scatter_A_B.svg")); # hide

# \fig{scatter_A_B.svg}

# Ricalcoliamo qui i coefficienti di correlazione nel caso in cui
# l'esperimento sia rifatto 10.000 volte. Notate che creo di nuovo un
# generatore di numeri casuali.

glc = GLC(1)
(n1_simul, n2_simul, A_simul, B_simul) = simulate_experiment(glc, 10_000)
println("Correlazione tra n1 e n2: ", corr(n1_simul, n2_simul))
println("Correlazione tra A e B: ", corr(A_simul, B_simul))

# ## Esercizio 12.1 ‚Äî Attrito viscoso (facoltativo)
#
# L'esercizio 12.1 √® preso da un vecchio tema d'esame, e va svolto in modo molto
# simile al precedente. Si tratta di misurare il coefficiente di viscosit√†
# $\eta$ partendo dalla velocit√† di caduta di una sferetta di metallo
# all'interno di un cilindro pieno di glicerina, tramite la formula $$ v_L =
# \frac{2R^2}{9\eta}(\rho - \rho_0) g = \frac{\Delta x}{\Delta t}, $$ dove
# $\Delta x$ √® la lunghezza del tratto percorso in caduta dalla sferetta e
# $\Delta t$ il tempo impiegato. La relazione si inverte facilmente per dare $$
# \eta = \frac{2R^2\,g\,\Delta t}{9\,\Delta x}(\rho - \rho_0), $$ dove le
# quantit√† misurate in ognuno degli esperimenti Monte Carlo sono $R$, $\Delta x
# = x_1 - x_0$, e $\Delta t$.
#
# Definiamo le costanti numeriche del problema, usando ancora Unitful.jl:

Œ¥t, Œ¥x, Œ¥R = 0.01s, 0.001m, 0.0001m;
œÅ, œÅ0 = 2700.0u"kg/m^3", 1250.0u"kg/m^3";
g = 9.81u"m/s^2";
Œ∑_true = 0.83u"kg/m/s";
R_true = [0.01m, 0.005m];
x0 = 20cm;
x1 = 60cm;
Œîx_true = x1 - x0;

# Definiamo anche alcune relazioni matematiche.

v_L(R, Œ∑) = 2R^2 / (9Œ∑) * (œÅ - œÅ0) * g;
Œît(R, Œîx, Œ∑) = Œîx / v_L(R, Œ∑);
Œît_true = [Œît(R, Œîx_true, Œ∑_true) for R in R_true];
Œ∑(R, Œît, Œîx) = 2R^2 * g * Œît / (9Œîx) * (œÅ - œÅ0);

# Definiamo ora la funzione `simulate`, che effettua _due_ esperimenti: uno con
# $R = 0.01\,\text{m}$ e l'altro con $R = 0.005\,\text{m}$.

function simulate(glc::GLC, Œ¥x, Œ¥t, Œ¥R)
    ## Misura dell'altezza iniziale
    cur_x0 = randgauss(glc, x0, Œ¥x)
    ## Misura dell'altezza finale
    cur_x1 = randgauss(glc, x1, Œ¥x)

    ## Questo array di 2 elementi conterr√† le due stime di Œ∑
    ## (corrispondenti ai due possibili raggi della sferetta)
    estimated_Œ∑ = zeros(typeof(Œ∑_true), 2)
    for case in [1, 2]
        ## Misura delle dimensioni della sferetta
        cur_R = randgauss(glc, R_true[case], Œ¥R)
        cur_Œîx = cur_x1 - cur_x0

        ## Misura del tempo necessario per cadere da cur_x0 a cur_x1
        cur_Œît = randgauss(glc, Œît_true[case], Œ¥t)

        ## Stima di Œ∑
        estimated_Œ∑[case] = Œ∑(cur_R, cur_Œît, cur_Œîx)
    end

    estimated_Œ∑
end

# Eseguiamo ora 1000 simulazioni e facciamo l'istogramma della stima di $\eta$
# per i due raggi della sferetta.

N = 1_000
glc = GLC(1)

Œ∑1 = Array{typeof(Œ∑_true)}(undef, N)
Œ∑2 = Array{typeof(Œ∑_true)}(undef, N)
for i in 1:N
    (Œ∑1[i], Œ∑2[i]) = simulate(glc, Œ¥x, Œ¥t, Œ¥R)
end

histogram(Œ∑2, label="R = $(R_true[2])")
histogram!(Œ∑1, label="R = $(R_true[1])");
savefig(joinpath(@OUTPUT, "hist_eta1_eta2.svg")); # hide

# \fig{hist_eta1_eta2.svg}

# Si tratta ora di stimare le incertezze di $\eta$ al variare degli
# errori considerati. Notate che per usare `round` con quantit√†
# associate ad unit√† di misura √® necessario specificare l'unit√† di
# misura usata per arrotondare: con 4 cifre, il valore `1 m` potrebbe
# essere scritto come `1.0000 m` oppure `100.0000 cm`!

## In Œ∑1 ed Œ∑2 abbiamo gi√† le stime di Œ∑ considerando tutti
## e tre gli errori
println("Tutti gli errori: Œ¥Œ∑(R1) = ", round(u"kg/m/s", std(Œ∑1), digits = 4))
println("                    (R2) = ", round(u"kg/m/s", std(Œ∑2), digits = 4))

## Ora dobbiamo eseguire di nuovo N esperimenti, assumendo che
## l'errore sia presente in una sola delle tre quantit√†
for i in 1:N
    (Œ∑1[i], Œ∑2[i]) = simulate(glc, 0.0m, 0.0s, Œ¥R)
end
println("Solo Œ¥R:          Œ¥Œ∑(R1) = ", round(u"kg/m/s", std(Œ∑1), digits = 4))
println("                    (R2) = ", round(u"kg/m/s", std(Œ∑2), digits = 4))

## Idem
for i in 1:N
    (Œ∑1[i], Œ∑2[i]) = simulate(glc, 0.0m, Œ¥t, 0.0m)
end
println("Solo Œ¥t:          Œ¥Œ∑(R1) = ", round(u"kg/m/s", std(Œ∑1), digits = 4))
println("                    (R2) = ", round(u"kg/m/s", std(Œ∑2), digits = 4))

## Idem
for i in 1:N
    (Œ∑1[i], Œ∑2[i]) = simulate(glc, Œ¥x, 0.0s, 0.0m)
end
println("Solo Œ¥x:          Œ¥Œ∑(R1) = ", round(u"kg/m/s", std(Œ∑1), digits = 4))
println("                    (R2) = ", round(u"kg/m/s", std(Œ∑2), digits = 4))
